from http.server import BaseHTTPRequestHandler
import json
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

class handler(BaseHTTPRequestHandler):
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
    model = None
    tokenizer = None
    
    @classmethod
    def load_model(cls):
        if cls.model is None:
            try:
                print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
                base_model_name = "ai-forever/rugpt3small_based_on_gpt2"
                cls.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    low_cpu_mem_usage=True
                )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∞—à –∞–¥–∞–ø—Ç–µ—Ä
                cls.model = PeftModel.from_pretrained(base_model, "./final_model")
                cls.model.eval()
                
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                return True
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                return False
        return True

    def do_POST(self):
        if self.path == '/api/generate':
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ
                if not self.load_model():
                    self.send_error(500, "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    return
                
                # –ß–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å
                content_length = int(self.headers['Content-Length'])
                post_data = self.rfile.read(content_length)
                data = json.loads(post_data.decode('utf-8'))
                
                text = data.get('text', '')
                question_count = data.get('questionCount', 5)
                difficulty = data.get('difficulty', '–ª–µ–≥–∫–∏–π')
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                prompt = f"–°–æ–∑–¥–∞–π —Ç–µ—Å—Ç —Å {question_count} –≤–æ–ø—Ä–æ—Å–∞–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ '{difficulty}' –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É —Ç–µ–∫—Å—Ç—É:\n\n{text}\n\n–¢–µ—Å—Ç:"
                
                # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
                inputs = self.tokenizer(
                    prompt, 
                    return_tensors="pt", 
                    max_length=512, 
                    truncation=True
                )
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs.input_ids,
                        max_length=1024,
                        num_beams=3,
                        temperature=0.8,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id,
                        repetition_penalty=1.2
                    )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
                result = generated_text.replace(prompt, "").strip()
                
                if not result:
                    result = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ç–µ–∫—Å—Ç."
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                
                response = {
                    "success": True,
                    "generatedTest": result,
                    "message": "–¢–µ—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ"
                }
                
                self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                self.send_error(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")
        else:
            self.send_error(404, "Not Found")

    def do_OPTIONS(self):
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()